---
title: "Project 6: Randomization and Matching"
author: "Takun Wang"
date: "2024-04-04"
output: 
  pdf_document: 
    highlight: tango  # default
    df_print: kable
    fig_width: 6      # default=6.5
    fig_height: 3.5     # default=4.5
urlcolor: blue
---

# 1. Introduction

In this project, you will explore the question of whether college education causally affects political participation. Specifically, you will use replication data from [Who Matches? Propensity Scores and Bias in the Causal Eï¬€ects of Education on Participation](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1409483) by former Berkeley PhD students John Henderson and Sara Chatfield. Their paper is itself a replication study of [Reconsidering the Effects of Education on Political Participation](https://www.jstor.org/stable/10.1017/s0022381608080651) by Cindy Kam and Carl Palmer. In their original 2008 study, Kam and Palmer argue that college education has no effect on later political participation, and use the propensity score matching to show that pre-college political activity drives selection into college and later political participation. Henderson and Chatfield in their 2011 paper argue that the use of the propensity score matching in this context is inappropriate because of the bias that arises from small changes in the choice of variables used to model the propensity score. They use [genetic matching](http://sekhon.berkeley.edu/papers/GenMatch.pdf) (at that point a new method), which uses an approach similar to optimal matching to optimize Mahalanobis distance weights. Even with genetic matching, they find that balance remains elusive however, thus leaving open the question of whether education causes political participation.

You will use these data and debates to investigate the benefits and pitfalls associated with matching methods. Replication code for these papers is available online, but as you'll see, a lot has changed in the last decade or so of data science! Throughout the assignment, use tools we introduced in lab from the [tidyverse](https://www.tidyverse.org/) and the [MatchIt](https://cran.r-project.org/web/packages/MatchIt/MatchIt.pdf) packages. Specifically, try to use dplyr, tidyr, purrr, stringr, and ggplot instead of base R functions. While there are other matching software libraries available, MatchIt tends to be the most up to date and allows for consistent syntax.


# 2. Data

The data is drawn from the [Youth-Parent Socialization Panel Study](https://www.icpsr.umich.edu/web/ICPSR/studies/4023/datadocumentation#) which asked students and parents a variety of questions about their political participation. This survey was conducted in several waves. The first wave was in 1965 and established the baseline pre-treatment covariates. The treatment is whether the student attended college between 1965 and 1973 (the time when the next survey wave was administered). The outcome is an index that calculates the number of political activities the student engaged in after 1965. Specifically, the key variables in this study are:
  
- **college**: Treatment of whether the student attended college or not. 1 if the student attended college between 1965 and 1973, 0 otherwise.  
- **ppnscal**: Outcome variable measuring the number of political activities the student participated in. Additive combination of whether the student voted in 1972 or 1980 (student_vote), attended a campaign rally or meeting (student_meeting), wore a campaign button (student_button), donated money to a campaign (student_money), communicated with an elected official (student_communicate), attended a demonstration or protest (student_demonstrate), was involved with a local community event (student_community), or some other political participation (student_other)

Otherwise, we also have covariates measured for survey responses to various questions about political attitudes. We have covariates measured for the students in the baseline year, covariates for their parents in the baseline year, and covariates from follow-up surveys. **Be careful here**. In general, post-treatment covariates will be clear from the name (i.e. student_1973Married indicates whether the student was married in the 1973 survey). Be mindful that the baseline covariates were all measured in 1965, the treatment occurred between 1965 and 1973, and the outcomes are from 1973 and beyond. We will distribute the Appendix from Henderson and Chatfield that describes the covariates they used, but please reach out with any questions if you have questions about what a particular variable means.

```{r Setup, message=FALSE}

## Load packages
library(tidyverse)
library(cobalt)
library(MatchIt)
library(ggpubr)
library(grid)
library(gridExtra)

```

```{r Data, message=FALSE}

## Load data
ypsps <- read_csv('data/ypsps.csv')
dim(ypsps)
colnames(ypsps)

## Select variables (the same vars as Appendix A in Henderson & Chatfield 2009)
ypsps <- ypsps %>% select(
  ## ID, treatment, outcome (3)
  interviewid, college, student_ppnscal,  
  ## STU: cognitive ability (7)
  student_PubAff, student_Newspaper, student_Radio, student_Magazine, student_FamTalk, 
  student_FrTalk, student_AdultTalk, 
  ## STU: external efficacy (6)
  student_GovtOpinion, student_GovtCrook, student_GovtWaste, student_TrGovt, student_GovtSmart, 
  student_Govt4All,
  ## STU: personality characteristics (9)
  student_LifeWish, student_GLuck, student_FPlans, student_WinArg, student_StrOpinion, 
  student_MChange, student_TrOthers, student_OthHelp, student_OthFair,
  ## STU: civic participation (9)
  student_SchOfficer, student_SchPublish, student_Hobby, student_SchClub, student_OccClub, 
  student_NeighClub, student_RelClub, student_YouthOrg, student_MiscClub,
  ## STU: other (7)
  student_SPID, student_Knowledge, student_NextSch, student_Phone, student_Gen, 
  student_Race, student_GPA, 
  ## PAR: cognitive ability (4)
  parent_Newspaper, parent_Radio, parent_TV, parent_Magazine,
  ## PAR: external efficacy (6)
  parent_GovtOpinion, parent_GovtCrook, parent_GovtWaste, parent_TrGovt, parent_GovtSmart, 
  parent_Govt4All,
  ## PAR: personality characteristics (9)
  parent_LifeWish, parent_GLuck, parent_FPlans, parent_WinArg, parent_StrOpinion, 
  parent_MChange, parent_TrOthers, parent_OthHelp, parent_OthFair,
  ## PAR: civic participation (11)
  parent_ChurchOrg, parent_FratOrg, parent_ProOrg, parent_CivicOrg, parent_CLOrg, 
  parent_NeighClub, parent_SportClub, parent_InfClub, parent_FarmGr, parent_WomenClub, 
  parent_MiscClub,
  ## PAR: political participation (7)
  parent_Vote, parent_Persuade, parent_Rally, parent_OthAct, parent_PolClub, 
  parent_Button, parent_Money, 
  ## PAR: other (6)
  parent_SPID, parent_Knowledge, parent_Employ, parent_EducHH, parent_HHInc, 
  parent_OwnHome)

## 84 variables in total
dim(ypsps)

## Convert some vars into factor class (for other non-binary vars, assume the values 
## have a meaningful order and that the spacing between consecutive values is the same)
ypsps$student_Race <- as.factor(ypsps$student_Race)

```


# 3. Randomization

Matching is usually used in observational studies to to approximate random assignment to treatment. But could it be useful even in randomized studies? To explore the question do the following:

1. Generate a vector that randomly assigns each unit to either treatment or control.  
2. Choose a baseline covariate (for either the student or parent). A binary covariate is probably best for this exercise.  
3. Visualize the distribution of the covariate by treatment/control condition. Are treatment and control balanced on this covariate?  
4. Simulate the first 3 steps 10,000 times and visualize the distribution of treatment/control balance across the simulations.


```{r Rando}

## Set seed
set.seed(224)

## Generate a random vector and choose a baseline covariate
df <- data.frame(
  ran_treat = as.factor(sample(0:1,
                               size = nrow(ypsps),
                               replace = TRUE,
                               prob = c(0.50,0.50))),
  gender = ypsps$student_Gen)

## Visualize the distribution by treatment/control (ggplot)
ggplot(df, aes(x = as.factor(gender), group = ran_treat, fill = ran_treat)) +
  geom_bar(position = "dodge") + 
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5) + 
  facet_grid(cols = vars(ran_treat))

## Simulate this 1,000 times
diff <- rep(0, 1000)

for (i in c(1:1000)) {
  df <- data.frame(
    ran_treat = as.factor(sample(0:1,
                               size = nrow(ypsps),
                               replace = TRUE,
                               prob = c(0.50,0.50))),
    gender = ypsps$student_Gen)
  
  df <- df %>% group_by(ran_treat) %>% summarise(mean = mean(gender)) 
  
  diff[i] <- df[[2, "mean"]] - df[[1, "mean"]]
}

## Plot
hist(diff)

```

## 3.1. Questions

1. What do you see across your simulations? Why does independence of treatment assignment and baseline covariates not guarantee balance of treatment assignment and baseline covariates?

Your Answer: The histogram from the simulations illustrates variability in the distribution of male/female percentages between the two groups across different iterations. This variability arises from the probabilistic nature of random sampling. In essence, while independence ensures that treatment assignment is not systematically biased by baseline characteristics, it does not eliminate the randomness inherent in sampling. This randomness can lead to scenarios where, by chance alone, there are significant differences in baseline characteristics (such as gender distribution in this context) between treatment groups. Therefore, it's crucial to conduct and review balance checks after randomization and consider statistical techniques or design adjustments (e.g., stratification, matching) to mitigate potential imbalances in key baseline covariates.


# 4. Propensity Score Matching

## 4.1. One Model

Select covariates that you think best represent the "true" model predicting whether a student chooses to attend college, and estimate a propensity score model to calculate the Average Treatment Effect on the Treated (ATT). Plot the balance of the covariates. Report the balance of the p-scores across both the treatment and control groups, and using a threshold of standardized mean difference of p-score $\leq .1$, report the number of covariates that meet that balance threshold.

```{r PSM}

# (1) Construct a model and calculate the ATT

## Create a logistic regression model
glm(college ~ 
      student_Gen + student_Race + student_GPA + student_Newspaper + student_WinArg +
      parent_EducHH + parent_HHInc + parent_Employ + parent_OwnHome + parent_Newspaper,
    family = "binomial",
    data = ypsps) %>% 
  summary()

## Match it based on the model
m.out <- ypsps %>% 
  matchit(college ~ 
            student_Gen + student_Race + student_GPA + student_Newspaper + student_WinArg +
            parent_EducHH + parent_HHInc + parent_Employ + parent_OwnHome + parent_Newspaper,
          distance = "glm", 
          link = "logit",
          method = "nearest", 
          caliper = 0.1,
          data = .)
m.data <- match.data(m.out)

## Calculate the ATT
Y1 <- m.data %>% filter(college == 1) %>% summarise(mean(student_ppnscal)) %>% pull()
Y0 <- m.data %>% filter(college == 0) %>% summarise(mean(student_ppnscal)) %>% pull()
ATT <- Y1 - Y0
print(ATT)


# (2) Check

## Check region of common support 
plot(m.out, type = "jitter", interactive = FALSE) 

## Report the balance of the p-scores
bal.plot(m.out, var.name = "distance",
         mirror = TRUE, type = "histogram")

## Plot the balance for the covariates
love.plot(m.out, stars = "raw")

## Create covariate balance table
tb <- bal.tab(m.out, un = TRUE, disp = c("means"))
tb

## Report the overall balance and the proportion of covariates that meet the balance threshold
SMD <- tb[[1]] %>% select(Diff.Adj) %>% slice(2:n()) %>% pull()
cat("Out of the", length(SMD), "covariates,", sum(abs(SMD) <= 0.1), "of them have SMD less or equal to 0.1")
cat("The proportion is:", sum(abs(SMD) <= 0.1) / length(SMD) * 100, "%")

## Calculate the mean percent improvement in the standardized mean diff. 
imp_pct <- tb[[1]] %>% slice(2:n()) %>% 
  summarise(Diff.Un = mean(abs(Diff.Un)), Diff.Adj = mean(abs(Diff.Adj))) %>% 
  mutate(imp = -(Diff.Adj - Diff.Un)) %>% 
  mutate(imp_pct = imp / Diff.Un) %>% 
  pull(imp_pct)

cat("The mean percent improve in the SMD is:", round(imp_pct * 100, 2), "%")

```


## 4.2. Simulations

Henderson/Chatfield argue that an improperly specified propensity score model can actually *increase* the bias of the estimate. To demonstrate this, they simulate 800,000 different propensity score models by choosing different permutations of covariates. To investigate their claim, do the following:

1. Using as many simulations as is feasible (at least 10,000 should be ok, more is better!), randomly select the number of and the choice of covariates for the propensity score model. 

2. For each run, store the ATT, the proportion of covariates that meet the standardized mean difference $\leq .1$ threshold, and the mean percent improvement in the standardized mean difference. You may also wish to store the entire models in a list and extract the relevant attributes as necessary.  

3. Plot all of the ATTs against all of the balanced covariate proportions. You may randomly sample or use other techniques like transparency if you run into overplotting problems. Alternatively, you may use plots other than scatterplots, so long as you explore the relationship between ATT and the proportion of covariates that meet the balance threshold.  

4. Finally choose 10 random models and plot their covariate balance plots (you may want to use a library like [gridExtra](https://cran.r-project.org/web/packages/gridExtra/index.html) to arrange these)


```{r Simu, warning=FALSE}

# (1) Simulation

## Set seed
set.seed(224)

## Create a dataframe to save results
n <- 1000
df <- data.frame(ATT     = c(1:n), 
                 n_mat   = c(1:n),
                 pct_mat = c(1:n), 
                 n_cov   = c(1:n), 
                 pct_cov = c(1:n),
                 bal_pct = c(1:n), 
                 imp_pct = c(1:n))

## Run the loop
for (i in c(1:n)) {
  
  ## Randomly select features
  col <- colnames(ypsps)[4:84]  # vars to choose from
  n_cov <- sample(10:81, 1)     # number of vars to select
  col <- sample(col, n_cov)         # sample n vars
  
  ## Generate data
  data <- ypsps[, c("student_ppnscal", "college", col)]
  
  ## Match data
  m.out <- matchit(college ~ . - student_ppnscal,
                   distance = "glm", 
                   link = "logit",
                   method = "nearest", 
                   caliper = 0.1, 
                   data = data)
  m.data <- match.data(m.out)
  
  ## Calculate the ATT
  Y1 <- m.data %>% filter(college == 1) %>% summarise(mean(student_ppnscal)) %>% pull()
  Y0 <- m.data %>% filter(college == 0) %>% summarise(mean(student_ppnscal)) %>% pull()
  ATT <- Y1 - Y0
  
  ## Cases matched
  tb <- bal.tab(m.out, un = TRUE)
  n_mat <- tb[[2]]["Matched (ESS)", "Treated"]
  pct_mat <- n_mat / nrow(data %>% filter(college == 0))
  
  ## Covariate balance
  SMD <- tb[[1]] %>% select(Diff.Adj) %>% slice(2:n()) %>% pull()
  imp_pct <- tb[[1]] %>% slice(2:n()) %>% 
    summarise(Diff.Un = mean(abs(Diff.Un)), Diff.Adj = mean(abs(Diff.Adj))) %>% 
    mutate(imp = -(Diff.Adj - Diff.Un)) %>% 
    mutate(imp_pct = imp / Diff.Un) %>% 
    pull(imp_pct)
  
  ## Save the results of each iteration 
  df$ATT[i] <- ATT                                     # ATT
  df$n_mat[i] <- n_mat                                 # number of cases successfully matched
  df$pct_mat[i] <- pct_mat                             # proportion of cases successfully matched
  df$n_cov[i] <- n_cov                                 # number of covariates selected
  df$pct_cov[i] <- n_cov / 81                          # proportion of covariates selected
  df$bal_pct[i] <- sum(abs(SMD) <= 0.1) / length(SMD)  # proportion of balanced covariates
  df$imp_pct[i] <- imp_pct                             # mean percent balance improvement
}

head(df)


# (2) Plot ATT

## ATT vs. proportion of cases successfully matched
g11 <- df %>% ggplot(aes(y = ATT, x = pct_mat), data = .) + 
  geom_point() + 
  geom_smooth(formula = y ~ x, method = "loess") + 
  labs(title = '% of cases matched')

## ATT vs. proportion of covariates selected
g12 <- df %>% ggplot(aes(y = ATT, x = pct_cov), data = .) + 
  geom_point() + 
  geom_smooth(formula = y ~ x, method = "loess") + 
  labs(title = '% of covariates selected')

## ATT vs. proportion of balanced covariates
g13 <- df %>% ggplot(aes(y = ATT, x = bal_pct), data = .) + 
  geom_point() + 
  geom_smooth(formula = y ~ x, method = "loess") + 
  labs(title = '% of covariate balanced')

## ATT vs. proportion of mean percent balance improvement
g14 <- df %>% ggplot(aes(y = ATT, x = imp_pct), data = .) + 
  geom_point() + 
  geom_smooth(formula = y ~ x, method = "loess") + 
  labs(title = '% of SMD improvement')

## Arrange all plots
ggarrange(g11, g12, labels = c('(a)', '(b)')) 
ggarrange(g13, g14, labels = c('(c)', '(d)')) 

```


## 4.3. Questions

1. How many simulations resulted in models with a higher proportion of balanced covariates? Do you have any concerns about this?

Your Answer: In the set of 1,000 simulations, 337 resulted in fully balanced covariates. However, a notable observation from graph (c) is the significant variability in estimated ATTs when all of the covariates appear balanced. This variability could be attributed to the randomly selected number of covariates in each iteration; notably, complete balance can be easier to be achieved with a minimal selection of covariates. This raises a concern illustrated by graph (b): fewer covariates tend to yield higher ATT estimates, indicating the significance of the quantity of covariates used in matching methods on the reliability of ATT estimations.

```{r Simu Q1}

## Number of simulations resulted in all covariates being balanced
df %>% filter(bal_pct == 1) %>% nrow()

## Simulations with bad proportion of SMD improvement 
df %>% filter(imp_pct <= 0.65)

```


2. Analyze the distribution of the ATTs. Do you have any concerns about this distribution?

Your Answer: The observation that fewer selected covariates tend to result in higher ATT estimates suggests a need for further investigation. Simulations should be performed with a consistent number of selected covariates, albeit choosing different sets in each run. This approach would allow for a clearer understanding of the relationship between the proportion of balanced covariates, the mean percent of balance improvement, and their influence on ATT estimations. By standardizing the number of covariates across simulations, we can more accurately assess the effects of covariate balance on ATT variability.


3. Do your 10 randomly chosen covariate balance plots produce similar numbers on the same covariates? Is it a concern if they do not?

Your Answer: The ten randomly chosen covariate balance plots do not consistently yield similar metrics for the same covariates. This inconsistency is a potential concern, as it may indicate that certain covariates remain unbalanced post-matching. The ability to balance a covariate effectively can vary based on the matching algorithm employed, the interplay with other covariates, and additional factors. This variability underscores the importance of routinely checking covariate balance as a part of the matching process to ensure the reliability and validity of the treatment effect estimations.

```{r Simu Q3, warning=FALSE}

## Run a loop
g <- list()

for (i in c(1:10)) {
  
  ## Randomly select features
  col <- colnames(ypsps)[4:84]  # vars to choose from
  n_cov <- sample(10:81, 1)     # number of vars to select
  col <- sample(col, n_cov)     # sample vars
  
  ## Match data
  data <- ypsps[, c("student_ppnscal", "college", col)]
  m.out <- matchit(college ~ . - student_ppnscal,
                   distance = "glm", 
                   link = "logit",
                   method = "nearest", 
                   caliper = 0.1, 
                   data = data)
  
  ## Plot
  g[[i]] <- love.plot(m.out, 
                      drop.distance = TRUE,
                      abs = TRUE, 
                      limits = list(m = c(0, 1)),
                      size = 2, 
                      position = "none",
                      title = "")
}

## Arrange all plots
marrangeGrob(g, nrow = 2, ncol = 1, top = "")

```



# 5. Matching Algorithm of Your Choice

## 5.1. Simulate Alternative Model

Henderson/Chatfield propose using genetic matching to learn the best weights for Mahalanobis distance matching. Choose a matching algorithm other than the propensity score (you may use genetic matching if you wish, but it is also fine to use the greedy or optimal algorithms we covered in lab instead). Repeat the same steps as specified in Section 4.2 and answer the following questions:

```{r Forest, warning=FALSE}

# (1) Simulation

## Set seed
set.seed(224)

## Create a dataframe to save results
n <- 1000
df2 <- data.frame(ATT     = c(1:n), 
                  n_mat   = c(1:n),
                  pct_mat = c(1:n), 
                  n_cov   = c(1:n), 
                  pct_cov = c(1:n),
                  bal_pct = c(1:n), 
                  imp_pct = c(1:n))

## Run the loop
for (i in c(1:n)) {
  
  ## Randomly select features
  col <- colnames(ypsps)[4:84]  # vars to choose from
  n_cov <- sample(10:81, 1)     # number of vars to select
  col <- sample(col, n_cov)     # sample vars
  
  ## Generate data
  data <- ypsps[, c("student_ppnscal", "college", col)]
  
  ## Match data
  m.out <- matchit(college ~ . - student_ppnscal,
                   distance = "randomforest", 
                   method = "nearest", 
                   caliper = 0.1, 
                   data = data)
  m.data <- match.data(m.out)
  
  ## Calculate the ATT
  Y1 <- m.data %>% filter(college == 1) %>% summarise(mean(student_ppnscal)) %>% pull()
  Y0 <- m.data %>% filter(college == 0) %>% summarise(mean(student_ppnscal)) %>% pull()
  ATT <- Y1 - Y0
  
  ## Cases matched
  tb <- bal.tab(m.out, un = TRUE)
  n_mat <- tb[[2]]["Matched (ESS)", "Treated"]
  pct_mat <- n_mat / nrow(data %>% filter(college == 0))
  
  ## Covariate balance
  SMD <- tb[[1]] %>% select(Diff.Adj) %>% slice(2:n()) %>% pull()
  imp_pct <- tb[[1]] %>% slice(2:n()) %>% 
    summarise(Diff.Un = mean(abs(Diff.Un)), Diff.Adj = mean(abs(Diff.Adj))) %>% 
    mutate(imp = -(Diff.Adj - Diff.Un)) %>% 
    mutate(imp_pct = imp / Diff.Un) %>% 
    pull(imp_pct)
  
  ## Save the results of each iteration 
  df2$ATT[i] <- ATT                                     # ATT
  df2$n_mat[i] <- n_mat                                 # number of cases successfully matched
  df2$pct_mat[i] <- pct_mat                             # proportion of cases successfully matched
  df2$n_cov[i] <- n_cov                                 # number of covariates selected
  df2$pct_cov[i] <- n_cov / 81                          # proportion of covariates selected
  df2$bal_pct[i] <- sum(abs(SMD) <= 0.1) / length(SMD)  # proportion of balanced covariates
  df2$imp_pct[i] <- imp_pct                             # mean percent balance improvement
}

head(df2)


# (2) Plot ATT

## ATT vs. proportion of cases successfully matched
g21 <- df2 %>% ggplot(aes(y = ATT, x = pct_mat), data = .) + 
  geom_point() + 
  geom_smooth(formula = y ~ x, method = "loess") + 
  labs(title = '% of cases matched')

## ATT vs. proportion of covariates selected
g22 <- df2 %>% ggplot(aes(y = ATT, x = pct_cov), data = .) + 
  geom_point() + 
  geom_smooth(formula = y ~ x, method = "loess") + 
  labs(title = '% of covariates selected')

## ATT vs. proportion of balanced covariates
g23 <- df2 %>% ggplot(aes(y = ATT, x = bal_pct), data = .) + 
  geom_point() + 
  geom_smooth(formula = y ~ x, method = "loess") + 
  labs(title = '% of covariate balanced')

## ATT vs. proportion of mean percent balance improvement
g24 <- df2 %>% ggplot(aes(y = ATT, x = imp_pct), data = .) + 
  geom_point() + 
  geom_smooth(formula = y ~ x, method = "loess") + 
  labs(title = '% of SMD improvement')

## Arrange all plots
ggarrange(g21, g22, labels = c('(a)', '(b)')) 
ggarrange(g23, g24, labels = c('(c)', '(d)')) 

```


## 5.2. Questions

1. Does your alternative matching method have more runs with higher proportions of balanced covariates? 

Your Answer: The alternative matching method utilizing random forest did not achieve a higher proportion of balanced covariates compared to the baseline approach. Specifically, while the baseline method achieved a 100% balance in 337 out of 1,000 simulations, the random forest method resulted in only 51 out of 1,000 simulations achieving at least 90% balance. This reduced efficacy in achieving covariate balance with the random forest method might stem from its inherent characteristics, which typically require larger sample sizes to function optimally. Despite this, the trend that fewer selected covariates are associated with higher ATT estimates persisted, as demonstrated in the data where iterations with lesser improvements in balance also selected fewer covariates and reported higher ATT estimates.

```{r Forest Q1}

## Number of simulations resulted in all covariates being balanced
df2 %>% filter(bal_pct >= 0.9) %>% nrow()

## Simulations with bad proportion of SMD improvement 
df2 %>% filter(imp_pct <= 0.3)

```

2. Use a visualization to examine the change in the distribution of the percent improvement in balance in propensity score matching vs. the distribution of the percent improvement in balance in your new method. Which did better? Analyze the results in 1-2 sentences.

Your Answer: Visual comparison of the percent improvement in balance between propensity score matching and the new method reveals that the logistic regression-based propensity score matching yields superior covariate balance between treatment and control groups. The machine learning-based method, here random forest, demonstrated less improvement in balance, likely due to its higher data requirements. This is particularly evident in smaller sample sizes, where logistic regression maintains its effectiveness in improving covariate balance, as highlighted by the comparative graph below.

```{r Forest Q2}

# Visualize distributions of percent improvement
df$cat <- "PSM"
df2$cat <- "RF"
df3 <- rbind(df, df2)

ggplot(df3) + 
  geom_density(aes(x = imp_pct, color = cat)) +
  labs(x = "Percent Improvement")

```


# 6. Discussion Questions

1. Why might it be a good idea to do matching even if we have a randomized or as-if-random design?

Your Answer: Matching in the context of randomized or as-if-random designs can be particularly beneficial despite the fundamental assumption of randomization ensuring unbiased treatment assignment. The key reason lies in the potential for random sampling variability to produce imbalances in baseline covariates across groups purely by chance. Implementing matching techniques in these scenarios serves to mitigate such imbalances, thereby enhancing the precision of the estimated effects by reducing the standard errors. This approach ensures a more accurate and reliable assessment of the treatment effect, capitalizing on the strengths of both randomization and matching to address inherent sampling randomness. 

2. The standard way of estimating the propensity score is using a logistic regression to estimate probability of treatment. Given what we know about the curse of dimensionality, do you think there might be advantages to using other machine learning algorithms (decision trees, bagging/boosting forests, ensembles, etc.) to estimate propensity scores instead?

Your Answer: Reflecting on the simulation results, three critical considerations emerge. First, the efficacy of machine learning-based methods is heavily contingent on the availability of large datasets. In scenarios where sample sizes are small, these algorithms might not perform optimally due to their complexity and the curse of dimensionality. Second, the decision to employ machine learning algorithms over logistic regression can and should be grounded in empirical evidence. By assessing whether these algorithms indeed facilitate better covariate balance between treatment and control groups, researchers can make informed choices. Last, the choice also hinges on the domain-specific understanding of the treatment assignment mechanism. Logistic regression offers a high degree of interpretability by relying on specified functional forms that mirror theoretical understandings of the assigning process. Machine learning algorithms, in contrast, often trade off interpretability for flexibility, adapting to complex patterns in the data without predefined assumptions about the nature of the relationships between covariates and treatment assignment.
